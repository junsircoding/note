{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单线程爬虫\n",
    "\n",
    "单线程的爬虫速度太慢，对应的我们可以使用多线程或者是进程版本来实现。\n",
    "\n",
    "举个例子，抓取糗事百科热门栏目下的十三个url地址的段子内容，地址: https://www.qiushibaike.com/\n",
    "\n",
    "普通面向对象版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "class QiubaiSpider:\n",
    "    def __init__(self):\n",
    "        self.url_temp = \"https://www.qiushibaike.com/8hr/page/{}/\"\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X \\\n",
    "        10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"}\n",
    "\n",
    "    def get_url_list(self):  # 获取url列表\n",
    "        return [self.url_temp.format(i) for i in range(1, 14)]\n",
    "\n",
    "    def parse_url(self, url):  # 发送请求，获取响应\n",
    "        print(url)\n",
    "        return requests.get(url, headers=self.headers).content.decode()\n",
    "\n",
    "    def get_content_list(self, html_str):  # 提取段子\n",
    "        html = etree.HTML(html_str)\n",
    "        div_list = html.xpath(\"//div[@id='content-left']/div\")\n",
    "        content_list = []\n",
    "        for div in div_list:\n",
    "            content = {}\n",
    "            content[\"content\"] = div.xpath(\n",
    "                \".//div[@class='content']/span/text()\")\n",
    "            content_list.append(content)\n",
    "        return content_list\n",
    "\n",
    "    def save_content_list(self, content_list):  # 保存数据\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        # 1. url_list\n",
    "        url_list = self.get_url_list()\n",
    "        # 2. 遍历，发送请求\n",
    "        for url in url_list:\n",
    "            html_str = self.parse_url(url)\n",
    "            # 3. 提取数据\n",
    "            content_list = self.get_content_list(html_str)\n",
    "            # 4. 保存\n",
    "            self.save_content_list(content_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    qiubai = QiubaiSpider()\n",
    "    qiubai.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多线程爬虫\n",
    "但是类似的单线程程序太慢，对应的可以考虑多线程实现，四个函数使用多个线程实现，分别使用三个队列存放数据  \n",
    "代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "\n",
    "class Qiubai:\n",
    "    def __init__(self):\n",
    "        self.temp_url = \"https://www.qiushibaike.com/8hr/page/{}/\"\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X \\\n",
    "        10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"}\n",
    "        self.url_queue = Queue()\n",
    "        self.html_queue = Queue()\n",
    "        self.content_list_queue = Queue()\n",
    "\n",
    "    def get_url_list(self):  # 获取url列表\n",
    "        for i in range(1, 14):\n",
    "            self.url_queue.put(self.temp_url.format(i))\n",
    "\n",
    "    def parse_url(self):\n",
    "        while True:  # 在这里使用，子线程不会结束，把子线程设置为守护线程\n",
    "            url = self.url_queue.get()\n",
    "            print(url)\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            self.html_queue.put(response.content.decode())\n",
    "            self.url_queue.task_done()\n",
    "\n",
    "    def get_content_list(self):  # 提取数据\n",
    "        while True:\n",
    "            html_str = self.html_queue.get()\n",
    "            html = etree.HTML(html_str)\n",
    "            div_list = html.xpath(\"//div[@id='content-left']/div\")\n",
    "            content_list = []\n",
    "            for div in div_list:\n",
    "                content = {}\n",
    "                content[\"content\"] = div.xpath(\n",
    "                    \".//div[@class='content']/span/text()\")\n",
    "                content_list.append(content)\n",
    "            self.content_list_queue.put(content_list)\n",
    "            self.html_queue.task_done()\n",
    "\n",
    "    def save_content_list(self):\n",
    "        while True:\n",
    "            content_list = self.content_list_queue.get()\n",
    "            pass\n",
    "            self.content_list_queue.task_done()\n",
    "\n",
    "    def run(self):\n",
    "        thread_list = []\n",
    "        # 1.url_list\n",
    "        t_url = threading.Thread(target=self.get_url_list)\n",
    "        thread_list.append(t_url)\n",
    "        # 2.遍历，发送请求，\n",
    "        for i in range(3):  # 三个线程发送请求\n",
    "            t_parse = threading.Thread(target=self.parse_url)\n",
    "            thread_list.append(t_parse)\n",
    "        # 3.提取数据\n",
    "        t_content = threading.Thread(target=self.get_content_list)\n",
    "        thread_list.append(t_content)\n",
    "        # 4.保存\n",
    "        t_save = threading.Thread(target=self.save_content_list)\n",
    "        thread_list.append(t_save)\n",
    "\n",
    "        for t in thread_list:\n",
    "            t.setDaemon(True)  # 把子线程设置为守护线程，当前这个线程不重要，主线程结束，子线程技术\n",
    "            t.start()\n",
    "\n",
    "        for q in [self.url_queue, self.html_queue, self.content_list_queue]:\n",
    "            q.join()  # 让主线程阻塞，等待队列的计数为0，\n",
    "\n",
    "        print(\"主线程结束\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    qiubai = Qiubai()\n",
    "    qiubai.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码中，`put` 会让队列的计数 +1，但是单纯的使用 `get` 不会让其-1，需要和 `task_done` 同时使用才能够-1；同时 `task_done` 不能放在另一个队列的 `put` 之前，否则可能会出现数据没有处理完成，程序结束的情况\n",
    "\n",
    "## 多进程爬虫\n",
    "\n",
    "这种方式由于 GIL 全局锁的存在，多线程在 Python3 下可能只是个摆设，对应的解释器执行其中的内容的时候仅仅是顺序执行，此时我们可以考虑多进程的方式实现，思路和多线程相似，只是对应的 api 不相同。\n",
    "具体的实现如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import threading\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import JoinableQueue as Queue\n",
    "\n",
    "\n",
    "class QiubaiSpider:\n",
    "    def __init__(self):\n",
    "        self.url_temp = \"https://www.qiushibaike.com/8hr/page/{}/\"\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"}\n",
    "        self.url_queue = Queue()  # 保存url\n",
    "        self.html_queue = Queue()  # 保存html字符串\n",
    "        self.content_queue = Queue()  # 保存提取到的数据\n",
    "\n",
    "    def get_url_list(self):\n",
    "        for i in range(1, 14):\n",
    "            self.url_queue.put(self.url_temp.format(i))\n",
    "\n",
    "    def parse_url(self):\n",
    "        while True:\n",
    "            url = self.url_queue.get()\n",
    "            print(url)\n",
    "            html_str = requests.get(url, headers=self.headers).content.decode()\n",
    "            self.html_queue.put(html_str)\n",
    "            self.url_queue.task_done()\n",
    "\n",
    "    def get_content_list(self):\n",
    "        while True:\n",
    "            html_str = self.html_queue.get()\n",
    "            html = etree.HTML(html_str)\n",
    "            div_list = html.xpath(\"//div[@id='content-left']/div\")\n",
    "            content_list = []\n",
    "            for div in div_list:\n",
    "                content = {}\n",
    "                content[\"content\"] = div.xpath(\n",
    "                    \".//div[@class='content']/span/text()\")\n",
    "                content_list.append(content)\n",
    "            self.content_queue.put(content_list)\n",
    "            self.html_queue.task_done()\n",
    "\n",
    "    def save_content_list(self):\n",
    "        while True:\n",
    "            content_list = self.content_queue.get()\n",
    "            pass\n",
    "            self.content_queue.task_done()\n",
    "\n",
    "    def run(self):\n",
    "        process_list = []\n",
    "        # 1. url_list\n",
    "        t_url = Process(target=self.get_url_list)\n",
    "        process_list.append(t_url)\n",
    "        # 2. 遍历，发送请求\n",
    "        for i in range(5):  # 创建5个子进程\n",
    "            t_parse = Process(target=self.parse_url)\n",
    "            process_list.append(t_parse)\n",
    "        # 3. 提取数据\n",
    "        t_content = Process(target=self.get_content_list)\n",
    "        process_list.append(t_content)\n",
    "        # 4. 保存\n",
    "        t_save = Process(target=self.save_content_list)\n",
    "        process_list.append(t_save)\n",
    "\n",
    "        for t in process_list:\n",
    "            t.daemon = True  # 把进线程设置为守护线程，主进程技术，子进程结束\n",
    "            t.start()\n",
    "\n",
    "        for q in [self.url_queue, self.html_queue, self.content_queue]:\n",
    "            q.join()  # 让主进程阻塞\n",
    "\n",
    "        print(\"主进程结束\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    qiubai = QiubaiSpider()\n",
    "    qiubai.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述多进程实现的代码中，multiprocessing 提供的 JoinableQueue 可以创建可连接的共享进程队列。和普通的 Queue 对象一样，队列允许项目的使用者通知生产者项目已经被成功处理。通知进程是使用共享的信号和条件变量来实现的。 对应的该队列能够和普通队列一样能够调用 task_done 和 join 方法"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3160b320a3d098505f6069b13711aa208a544b025e74a1c73de7d3971407adcc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
