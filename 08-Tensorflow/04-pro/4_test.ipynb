{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方入门教程第四节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-02 14:24:38--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.43.16, 142.251.42.240, 172.217.160.112, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.43.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 200682221 (191M) [application/zip]\n",
      "Saving to: ‘./rps.zip’\n",
      "\n",
      "./rps.zip           100%[===================>] 191.38M  10.1MB/s    in 21s     \n",
      "\n",
      "2022-05-02 14:25:00 (9.06 MB/s) - ‘./rps.zip’ saved [200682221/200682221]\n",
      "\n",
      "--2022-05-02 14:25:00--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.42.240, 142.251.43.16, 172.217.160.112, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.42.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29516758 (28M) [application/zip]\n",
      "Saving to: ‘./rps-test-set.zip’\n",
      "\n",
      "./rps-test-set.zip  100%[===================>]  28.15M  9.09MB/s    in 3.1s    \n",
      "\n",
      "2022-05-02 14:25:04 (9.09 MB/s) - ‘./rps-test-set.zip’ saved [29516758/29516758]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
    "        -O ./rps.zip\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n",
    "        -O ./rps-test-set.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = \"./rps.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
    "zip_ref.extractall(\"./\")\n",
    "zip_ref.close()\n",
    "\n",
    "local_zip = \"./rps-test-set.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
    "zip_ref.extractall(\"./\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"./rps/\"\n",
    "training_datagen = kr.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size = (150, 150),\n",
    "    class_mode = \"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 372 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "TEST_TRAINING_DIR = \"./rps-test-set/\"\n",
    "validation_datagen = kr.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "train_generator = validation_datagen.flow_from_directory(\n",
    "    TEST_TRAINING_DIR,\n",
    "    target_size = (150, 150),\n",
    "    class_mode = \"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 14:46:05.480537: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-02 14:46:05.480677: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = kr.Sequential([\n",
    "    kr.layers.Conv2D(64, (3, 3), activation=tf.nn.relu, input_shape=(150, 150, 3)),\n",
    "    kr.layers.MaxPool2D(2, 2),\n",
    "    kr.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    kr.layers.MaxPool2D(2, 2),\n",
    "    kr.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    kr.layers.MaxPool2D(2, 2),\n",
    "    kr.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    kr.layers.MaxPool2D(2, 2),\n",
    "\n",
    "    kr.layers.Flatten(),\n",
    "    kr.layers.Dropout(0.5),\n",
    "\n",
    "    kr.layers.Dense(512, activation=tf.nn.relu),\n",
    "    kr.layers.Dense(3, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9996 - accuracy: 0.5484"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'keras.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/junsircoding/Coding/note/tensorflow练习/youtube教程/4_test.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/junsircoding/Coding/note/tensorflow%E7%BB%83%E4%B9%A0/youtube%E6%95%99%E7%A8%8B/4_test.ipynb#ch0000008?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/junsircoding/Coding/note/tensorflow%E7%BB%83%E4%B9%A0/youtube%E6%95%99%E7%A8%8B/4_test.ipynb#ch0000008?line=1'>2</a>\u001b[0m     train_generator,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/junsircoding/Coding/note/tensorflow%E7%BB%83%E4%B9%A0/youtube%E6%95%99%E7%A8%8B/4_test.ipynb#ch0000008?line=2'>3</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/junsircoding/Coding/note/tensorflow%E7%BB%83%E4%B9%A0/youtube%E6%95%99%E7%A8%8B/4_test.ipynb#ch0000008?line=3'>4</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_datagen,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/junsircoding/Coding/note/tensorflow%E7%BB%83%E4%B9%A0/youtube%E6%95%99%E7%A8%8B/4_test.ipynb#ch0000008?line=4'>5</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/junsircoding/Coding/note/tensorflow%E7%BB%83%E4%B9%A0/youtube%E6%95%99%E7%A8%8B/4_test.ipynb#ch0000008?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py:984\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=980'>981</a>\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=981'>982</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=982'>983</a>\u001b[0m   \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=983'>984</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=984'>985</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=985'>986</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=986'>987</a>\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=987'>988</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=988'>989</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=989'>990</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=990'>991</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=991'>992</a>\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///Users/junsircoding/miniforge3/envs/tensor_env/lib/python3.8/site-packages/keras/engine/data_adapter.py?line=992'>993</a>\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'keras.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=validation_datagen,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = \"\"\n",
    "model.predict(images, batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4bdecafe0dca4671be1f330bb66afefe9717e3009a015ec19bd1158a172ba6f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensor_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
