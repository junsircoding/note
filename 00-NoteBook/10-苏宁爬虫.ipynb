{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 苏宁图书爬虫的 Spider 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class SuningSpider(scrapy.Spider):\n",
    "    name = 'suning'\n",
    "    allowed_domains = ['suning.com']\n",
    "    start_urls = ['https://book.suning.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 现获取大分类的分组\n",
    "        b_div_list = response.xpath(\n",
    "            \"//div[@class='menu-list']/div[@class='menu-item']\")\n",
    "        # 中间分类的分组\n",
    "        m_div_list = response.xpath(\"//div[@class='menu-sub']\")\n",
    "        for div in b_div_list:\n",
    "            item = {}\n",
    "            item[\"b_cate\"] = div.xpath(\n",
    "                \".//h3/a/text()\").extract_first()  # 大分类的名字\n",
    "            index_now = b_div_list.index(div)  # 确定当前是第一个大分类\n",
    "            now_menu_sub_div = m_div_list[index_now]  # 确定大分类对应的中间分类和小分类\n",
    "            submenu_item_div = now_menu_sub_div.xpath(\n",
    "                \".//div[@class='submenu-left']/p[@class='submenu-item']\")  # 获取的是中间分类的分组\n",
    "            for div in submenu_item_div:\n",
    "                item[\"m_cate\"] = div.xpath(\n",
    "                    \".//a/text()\").extract_first()  # 中间分类\n",
    "                li_list = div.xpath(\"./following-sibling::ul[1]/li\")  # 小分类的分组\n",
    "                for li in li_list:\n",
    "                    item[\"s_cate_href\"] = li.xpath(\n",
    "                        './a/@href').extract_first()  # 小分类的url地址\n",
    "                    item[\"s_cate\"] = li.xpath(\n",
    "                        './a/text()').extract_first()  # 小分类的文本\n",
    "                    # print(item)\n",
    "                    yield scrapy.Request(  # 获取列表页的第一部分内容\n",
    "                        item[\"s_cate_href\"],\n",
    "                        callback=self.parse_book_list,\n",
    "                        meta={\"item\": deepcopy(item)}\n",
    "                    )\n",
    "\n",
    "                    # 获取列表页第一页的后一部分内容\n",
    "                    next_part_url = \"https://list.suning.com/emall/showProductList.do?ci={}&pg=03&cp=0&il=0&iy=0&adNumber=0&n=1&ch=4&sesab=ABBAAA&id=IDENTIFYING&cc=010&paging=1&sub=0\"\n",
    "                    ci = item[\"s_cate_href\"].split(\"-\")[1]\n",
    "                    next_part_url = next_part_url.format(ci)\n",
    "                    yield scrapy.Request(\n",
    "                        next_part_url,\n",
    "                        meta={\"item\": deepcopy(item)},\n",
    "                        callback=self.parse_book_list\n",
    "                    )\n",
    "\n",
    "    def parse_book_list(self, response):\n",
    "        item = response.meta[\"item\"]\n",
    "        # 首页请求获取前一部分的数据\n",
    "        li_list = response.xpath(\n",
    "            \"//div[@id='filter-results']//li[contains(@class,product)]\")\n",
    "        if len(li_list) == 0:  # 获取后一部分的数据\n",
    "            li_list = response.xpath(\"//li[contains(@class,product)]\")\n",
    "\n",
    "        for li in li_list:\n",
    "            item[\"book_title\"] = li.xpath(\n",
    "                \".//p[@class='sell-point']/a/text()\").extract_first().strip()\n",
    "            item[\"book_href\"] = li.xpath(\n",
    "                \".//p[@class='sell-point']/a/@href\").extract_first().strip()\n",
    "            yield response.follow(\n",
    "                item[\"book_href\"],\n",
    "                callback=self.parse_book_detail,\n",
    "                meta={\"item\": deepcopy(item)}\n",
    "            )\n",
    "\n",
    "        # TODO 翻页\n",
    "        next_url_temp_1 = \"https://list.suning.com/emall/showProductList.do?ci={}&pg=03&cp={}&il=0&iy=0&adNumber=0&n=1&ch=4&sesab=ABBAAA&id=IDENTIFYING&cc=010\"\n",
    "        next_url_temp_2 = \"https://list.suning.com/emall/showProductList.do?ci={}&pg=03&cp={}&il=0&iy=0&adNumber=0&n=1&ch=4&sesab=ABBAAA&id=IDENTIFYING&cc=010&paging=1&sub=0\"\n",
    "        ci = item[\"s_cate_href\"].split(\"-\")[1]\n",
    "        current_page = re.findall(\n",
    "            'param.currentPage = \"(.*?)\";', response.body.decode())[0]  # 提取当前页码数\n",
    "        total_page = re.findall(\n",
    "            'param.pageNumbers = \"(.*?)\";', response.body.decode())[0]\n",
    "        if int(current_page) < int(total_page):\n",
    "            next_page_num = int(current_page)+1\n",
    "            next_url_1 = next_url_temp_1.format(ci, next_page_num)  # 数据的前一部分地址\n",
    "            next_url_2 = next_url_temp_2.format(ci, next_page_num)  # 数据的前一部分地址\n",
    "            # 构造前半部分数据的请求\n",
    "            # print(\">\"*100)\n",
    "            yield scrapy.Request(\n",
    "                next_url_1,\n",
    "                callback=self.parse_book_list,\n",
    "                meta={\"item\": item}\n",
    "            )\n",
    "            # print(\"?\"*100)\n",
    "            # 构造后半部分数据的请求\n",
    "            yield scrapy.Request(\n",
    "                next_url_2,\n",
    "                callback=self.parse_book_list,\n",
    "                meta={\"item\": item}\n",
    "            )\n",
    "\n",
    "    def parse_book_detail(self, response):  # 获取详情页，提取字段组装价格的url地址\n",
    "        item = response.meta[\"item\"]\n",
    "        price_url_temp = \"https://pas.suning.com/nspcsale_0_000000000{}_000000000{}_{}_10_010_0100101_226503_1000000_9017_10106____{}_{}.html?callback=pcData&_=1526011028849\"\n",
    "        p1 = response.url.split(\"/\")[-1].split(\".\")[0]\n",
    "        p3 = response.url.split(\"/\")[-2]\n",
    "        # \"catenIds\":\"R9011195\",\n",
    "        p4 = re.findall('\"catenIds\":\"(.*?)\",', response.body.decode())\n",
    "        if len(p4) > 0:\n",
    "            p4 = p4[0]\n",
    "            # \"weight\":\"0.5\",\n",
    "            p5 = re.findall('\"weight\":\"(.*?)\",', response.body.decode())[0]\n",
    "            price_url = price_url_temp.format(p1, p1, p3, p4, p5)\n",
    "            # print(price_url,\"*\"*100)\n",
    "            yield scrapy.Request(\n",
    "                price_url,\n",
    "                callback=self.parse_book_price,\n",
    "                meta={\"item\": item}\n",
    "            )\n",
    "\n",
    "    def parse_book_price(self, response):  # 获取价格\n",
    "        item = response.meta[\"item\"]\n",
    "        item[\"book_price\"] = re.findall(\n",
    "            '\"netPrice\":\"(.*?)\"', response.body.decode())[0]\n",
    "        print(item)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3160b320a3d098505f6069b13711aa208a544b025e74a1c73de7d3971407adcc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
